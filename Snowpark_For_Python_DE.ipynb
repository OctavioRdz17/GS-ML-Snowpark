{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "Perform data analysis and data preparation tasks to train a Linear Regression model to predict future ROI (Return On Investment) of variable ad spend budgets across multiple channels including search, video, social media, and email using Snowpark for Python, Snowpark ML and Streamlit. By the end of the session, you will have an interactive web application deployed visualizing the ROI of different allocated advertising spend budgets.\n",
    "### Data Engineering -- Data Analysis and Data Preparation\n",
    "\n",
    "In this Notebook, we will focus on Data Engineering in Snowflake using Snowpark for Python.\n",
    "\n",
    "- Establish secure connection to Snowflake\n",
    "- Load data from Snowflake tables into Snowpark DataFrames\n",
    "- Perform Exploratory Data Analysis on Snowpark DataFrames\n",
    "- Pivot and Join data from multiple tables using Snowpark DataFrames\n",
    "- Demostrate how to automate data preparation using Snowflake Tasks\n",
    "\n",
    "For environment setup including loading data into Snowflake tables, and step-by-step instructions, please refer to the [QuickStart Guide.](https://quickstarts.snowflake.com/guide/getting_started_with_dataengineering_ml_using_snowpark_python/index.html#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.functions import month,year,col,sum\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.task import Task, StoredProcedureCall\n",
    "from snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\n",
    "from snowflake.core import CreateMode\n",
    "\n",
    "# Misc\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Secure Connection to Snowflake\n",
    "\n",
    "Using the Snowpark Python API, itâ€™s quick and easy to establish a secure connection between Snowflake and Notebook.\n",
    "\n",
    "Note: Other connection options include Username/Password, MFA, OAuth, Okta, SSO\n",
    "\n",
    "TIP: Learn more about [Session object.](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : TRAININGSNOWFLAKE\n",
      "Role                        : \"ACCOUNTADMIN\"\n",
      "Database                    : \"DASH_DB\"\n",
      "Schema                      : \"DASH_SCHEMA\"\n",
      "Warehouse                   : \"DASH_L1\"\n",
      "Snowflake version           : 8.12.2\n",
      "Snowpark for Python version : 1.13.0\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load Aggregated Campaign Spend Data from Snowflake table into Snowpark DataFrame\n",
    "\n",
    "Let's first load the campaign spend data. This table contains ad click data that has been aggregated to show daily spend across digital ad channels including search engines, social media, email and video.\n",
    "\n",
    "Note: Some other ways to load data in a Snowpark DataFrame\n",
    "\n",
    "- session.sql(\"select col1, col2... from tableName\")\n",
    "- session.read.options({\"field_delimiter\": \",\", \"skip_header\": 1}).schema(user_schema).csv(\"@mystage/testCSV.csv\")\n",
    "- session.read.parquet(\"@stageName/path/to/file\")\n",
    "- session.create_dataframe([1,2,3], schema=[\"col1\"])\n",
    "\n",
    "TIP: Learn more about [Snowpark DataFrames.](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': ['SELECT  *  FROM (campaign_spend)'], 'post_actions': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_df_spend = session.table('campaign_spend')\n",
    "snow_df_spend.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions like show(), collect(), count() send the DataFrame SQL for execution on the server\n",
    "\n",
    "Note: History object provides the query ID which can be helpful for debugging as well as the SQL query executed on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------\n",
      "|\"CAMPAIGN\"              |\"CHANNEL\"      |\"DATE\"      |\"TOTAL_CLICKS\"  |\"TOTAL_COST\"  |\"ADS_SERVED\"  |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "|winter_sports           |video          |2012-06-03  |213             |1762          |426           |\n",
      "|sports_across_cultures  |video          |2012-06-02  |87              |678           |157           |\n",
      "|building_community      |search_engine  |2012-06-03  |66              |471           |134           |\n",
      "|world_series            |social_media   |2017-12-28  |72              |591           |149           |\n",
      "|winter_sports           |email          |2018-02-09  |252             |1841          |473           |\n",
      "|spring_break            |video          |2017-11-14  |162             |1155          |304           |\n",
      "|nba_finals              |email          |2017-11-22  |68              |480           |134           |\n",
      "|winter_sports           |social_media   |2018-03-10  |227             |1797          |454           |\n",
      "|spring_break            |search_engine  |2017-08-30  |150             |1226          |302           |\n",
      "|uefa                    |video          |2017-09-30  |81              |701           |168           |\n",
      "|uefa                    |video          |2018-01-23  |73              |545           |141           |\n",
      "|sports_across_cultures  |search_engine  |2017-10-12  |73              |544           |143           |\n",
      "|winter_sports           |social_media   |2018-01-14  |207             |1640          |418           |\n",
      "|youth_on_course         |search_engine  |2018-03-29  |164             |1036          |291           |\n",
      "|memorial_day            |social_media   |2018-01-18  |131             |1119          |281           |\n",
      "|family_history          |video          |2018-03-24  |88              |646           |166           |\n",
      "|memorial_day            |search_engine  |2017-12-09  |134             |968           |262           |\n",
      "|youth_in_action         |search_engine  |2018-03-24  |194             |1642          |411           |\n",
      "|winter_sports           |video          |2017-10-25  |208             |1673          |431           |\n",
      "|thanksgiving_football   |video          |2017-11-10  |82              |633           |165           |\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QueryRecord(query_id='01b36158-3201-27fa-0005-26be00089332', sql_text='SELECT  *  FROM campaign_spend LIMIT 20')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with session.query_history() as history:\n",
    "    snow_df_spend.show(20)\n",
    "history.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Spend per Year and Month For All Channels\n",
    "\n",
    "Let's transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions.\n",
    "\n",
    "TIP: For a full list of functions, refer to the [documentation.](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"CHANNEL\"      |\"TOTAL_COST\"  |\n",
      "---------------------------------------------------\n",
      "|2012    |5        |search_engine  |516431        |\n",
      "|2012    |5        |video          |516729        |\n",
      "|2012    |5        |email          |517208        |\n",
      "|2012    |5        |social_media   |517618        |\n",
      "|2012    |6        |video          |501098        |\n",
      "|2012    |6        |search_engine  |506497        |\n",
      "|2012    |6        |social_media   |504679        |\n",
      "|2012    |6        |email          |501947        |\n",
      "|2012    |7        |search_engine  |522780        |\n",
      "|2012    |7        |email          |518405        |\n",
      "---------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stats per Month per Channel\n",
    "snow_df_spend_per_channel = snow_df_spend.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n",
    "    with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n",
    "\n",
    "snow_df_spend_per_channel.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot on Channel: Total Spend Across All Channels\n",
    "\n",
    "Let's further transform the campaign spend data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions. This transformation will enable us to join with the revenue table such that we will have our input features and target variable in a single table for model training.\n",
    "\n",
    "TIP: For a full list of functions, refer to the [documentation.](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\n",
      "---------------------------------------------------------------------------\n",
      "|2022    |5        |197115           |195300          |196462   |195940   |\n",
      "|2022    |4        |503395           |504021          |503304   |503256   |\n",
      "|2022    |3        |519284           |520241          |522759   |520458   |\n",
      "|2022    |2        |468251           |471481          |470541   |472944   |\n",
      "|2022    |1        |520880           |524474          |519994   |520014   |\n",
      "|2021    |12       |518867           |520503          |520008   |518725   |\n",
      "|2021    |11       |505345           |501291          |503561   |504571   |\n",
      "|2021    |10       |524027           |520043          |520050   |520703   |\n",
      "|2021    |9        |503606           |504865          |502770   |505251   |\n",
      "|2021    |8        |521025           |522775          |519864   |519576   |\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_per_month = snow_df_spend_per_channel.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n",
    "snow_df_spend_per_month = snow_df_spend_per_month.select(\n",
    "    col(\"YEAR\"),\n",
    "    col(\"MONTH\"),\n",
    "    col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n",
    "    col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n",
    "    col(\"'video'\").as_(\"VIDEO\"),\n",
    "    col(\"'email'\").as_(\"EMAIL\")\n",
    ")\n",
    "snow_df_spend_per_month.sort(col(\"YEAR\"),col(\"MONTH\"),ascending=[0,0]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Transformed Data into Snowflake Table\n",
    "\n",
    "Let's save the transformed data into a Snowflake table SPEND_PER_MONTH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df_spend_per_month.write.mode('overwrite').save_as_table('SPEND_PER_MONTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation: Run Campaign Spend Data Transformations As a Snowflake Task\n",
    "\n",
    "Note: Optionally you can run all these transformations as an automated task by deploying the code to Snowflake as a Snowpark Stored Procedure and executing it as a Snowflake Task.\n",
    "\n",
    "TIP: Learn more about [Stored Procedures](https://docs.snowflake.com/en/sql-reference/stored-procedures-python) and [Snowflake Tasks](https://docs.snowflake.com/en/sql-reference/sql/create-task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def campaign_spend_data_pipeline(session: Session) -> str:\n",
    "  # DATA TRANSFORMATIONS\n",
    "  # Perform the following actions to transform the data\n",
    "\n",
    "  # Load the campaign spend data\n",
    "  snow_df_spend_t = session.table('campaign_spend')\n",
    "\n",
    "  # Transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions\n",
    "  snow_df_spend_per_channel_t = snow_df_spend_t.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n",
    "      with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n",
    "\n",
    "  # Transform the data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions\n",
    "  snow_df_spend_per_month_t = snow_df_spend_per_channel_t.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n",
    "  snow_df_spend_per_month_t = snow_df_spend_per_month_t.select(\n",
    "      col(\"YEAR\"),\n",
    "      col(\"MONTH\"),\n",
    "      col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n",
    "      col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n",
    "      col(\"'video'\").as_(\"VIDEO\"),\n",
    "      col(\"'email'\").as_(\"EMAIL\")\n",
    "  )\n",
    "\n",
    "  # Save transformed data\n",
    "  snow_df_spend_per_month_t.write.mode('overwrite').save_as_table('SPEND_PER_MONTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register data pipeline function as a task\n",
    "root = Root(session)\n",
    "my_task = Task(name='campaign_spend_data_pipeline_task'\n",
    "               , definition=StoredProcedureCall(\n",
    "                   campaign_spend_data_pipeline, stage_location='@dash_sprocs'\n",
    "               )\n",
    "               , warehouse='DASH_L1'\n",
    "               , schedule=timedelta(minutes=3))\n",
    "\n",
    "tasks = root.databases[session.get_current_database()].schemas[session.get_current_schema()].tasks\n",
    "task_res = tasks.create(my_task,mode=CreateMode.or_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default a Task is suspended and we need to resume it if we want it run based on the schema. Note that we can still execute a task by calling the execute method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Execute the task\n",
    "task_res.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Revenue per Year And Month\n",
    "Now let's load revenue table and transform the data into revenue per year/month using group_by() and agg() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"REVENUE\"   |\n",
      "---------------------------------\n",
      "|2012    |5        |3264300.11  |\n",
      "|2012    |6        |3208482.33  |\n",
      "|2012    |7        |3311966.98  |\n",
      "|2012    |8        |3311752.81  |\n",
      "|2012    |9        |3208563.06  |\n",
      "|2012    |10       |3334028.46  |\n",
      "|2012    |11       |3185894.64  |\n",
      "|2012    |12       |3334570.96  |\n",
      "|2013    |1        |3316455.44  |\n",
      "|2013    |2        |2995042.21  |\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_revenue = session.table('monthly_revenue')\n",
    "snow_df_revenue_per_month = snow_df_revenue.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n",
    "snow_df_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Total Spend and Total Revenue per Year and Month Across All Channels\n",
    "\n",
    "Next let's join this revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "|\"YEAR\"  |\"MONTH\"  |\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "----------------------------------------------------------------------------------------\n",
      "|2012    |5        |516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|2012    |6        |506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|2012    |7        |522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|2012    |8        |519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|2012    |9        |507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|2012    |10       |518942           |520863          |522768   |519950   |3334028.46  |\n",
      "|2012    |11       |505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|2012    |12       |520148           |520711          |521427   |520724   |3334570.96  |\n",
      "|2013    |1        |522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|2013    |2        |467736           |474679          |469856   |469784   |2995042.21  |\n",
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_and_revenue_per_month = snow_df_spend_per_month.join(snow_df_revenue_per_month, [\"YEAR\",\"MONTH\"])\n",
    "snow_df_spend_and_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >>>>>>>>>> Examine Snowpark DataFrame Query and Execution Plan <<<<<<<<<<\n",
    "\n",
    "Snowpark makes is really convenient to look at the DataFrame query and execution plan using explain() Snowpark DataFrame function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------DATAFRAME EXECUTION PLAN----------\n",
      "Query List:\n",
      "1.\n",
      "SELECT  *  FROM (( SELECT \"YEAR\" AS \"YEAR\", \"MONTH\" AS \"MONTH\", \"SEARCH_ENGINE\" AS \"SEARCH_ENGINE\", \"SOCIAL_MEDIA\" AS \"SOCIAL_MEDIA\", \"VIDEO\" AS \"VIDEO\", \"EMAIL\" AS \"EMAIL\" FROM ( SELECT \"YEAR\", \"MONTH\", \"'search_engine'\" AS \"SEARCH_ENGINE\", \"'social_media'\" AS \"SOCIAL_MEDIA\", \"'video'\" AS \"VIDEO\", \"'email'\" AS \"EMAIL\" FROM ( SELECT  *  FROM ( SELECT  *  FROM ( SELECT \"YEAR(DATE)\" AS \"YEAR\", \"MONTH(DATE)\" AS \"MONTH\", \"CHANNEL\", \"TOTAL_COST\" FROM ( SELECT year(\"DATE\") AS \"YEAR(DATE)\", month(\"DATE\") AS \"MONTH(DATE)\", \"CHANNEL\", sum(\"TOTAL_COST\") AS \"TOTAL_COST\" FROM ( SELECT  *  FROM campaign_spend) GROUP BY year(\"DATE\"), month(\"DATE\"), \"CHANNEL\")) ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST) PIVOT (sum(\"TOTAL_COST\") FOR \"CHANNEL\" IN ('search_engine', 'social_media', 'video', 'email'))) ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST)) AS SNOWPARK_LEFT INNER JOIN ( SELECT \"YEAR\" AS \"YEAR\", \"MONTH\" AS \"MONTH\", \"REVENUE\" AS \"REVENUE\" FROM ( SELECT \"YEAR\", \"MONTH\", \"SUM(REVENUE)\" AS \"REVENUE\" FROM ( SELECT \"YEAR\", \"MONTH\", sum(\"REVENUE\") AS \"SUM(REVENUE)\" FROM ( SELECT  *  FROM monthly_revenue) GROUP BY \"YEAR\", \"MONTH\") ORDER BY \"YEAR\" ASC NULLS FIRST, \"MONTH\" ASC NULLS FIRST)) AS SNOWPARK_RIGHT USING (YEAR, MONTH))\n",
      "Logical Execution Plan:\n",
      "GlobalStats:\n",
      "    partitionsTotal=2\n",
      "    partitionsAssigned=2\n",
      "    bytesAssigned=1731072\n",
      "Operations:\n",
      "1:0     ->Result  EXTRACT(year from CAMPAIGN_SPEND.DATE), EXTRACT(month from CAMPAIGN_SPEND.DATE), 'search_engine', 'social_media', 'video', 'email', SUM(MONTHLY_REVENUE.REVENUE)  \n",
      "1:1          ->InnerJoin  joinKey: (EXTRACT(month from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.MONTH) AND (EXTRACT(year from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.YEAR)  \n",
      "1:2               ->Sort  EXTRACT(year from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST, EXTRACT(month from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST  \n",
      "1:3                    ->Pivot  'search_engine', 'social_media', 'video', 'email'  \n",
      "1:4                         ->Sort  EXTRACT(year from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST, EXTRACT(month from CAMPAIGN_SPEND.DATE) ASC NULLS FIRST  \n",
      "1:5                              ->Aggregate  aggExprs: [SUM(CAMPAIGN_SPEND.TOTAL_COST)], groupKeys: [EXTRACT(year from CAMPAIGN_SPEND.DATE), EXTRACT(month from CAMPAIGN_SPEND.DATE), CAMPAIGN_SPEND.CHANNEL]  \n",
      "1:6                                   ->TableScan  DASH_DB.DASH_SCHEMA.CAMPAIGN_SPEND  CHANNEL, DATE, TOTAL_COST  {partitionsTotal=1, partitionsAssigned=1, bytesAssigned=1728512}\n",
      "1:7               ->Sort  MONTHLY_REVENUE.YEAR ASC NULLS FIRST, MONTHLY_REVENUE.MONTH ASC NULLS FIRST  \n",
      "1:8                    ->Aggregate  aggExprs: [SUM(MONTHLY_REVENUE.REVENUE)], groupKeys: [MONTHLY_REVENUE.YEAR, MONTHLY_REVENUE.MONTH]  \n",
      "1:9                         ->JoinFilter  joinKey: (EXTRACT(month from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.MONTH) AND (EXTRACT(year from CAMPAIGN_SPEND.DATE) = MONTHLY_REVENUE.YEAR)  \n",
      "1:10                              ->TableScan  DASH_DB.DASH_SCHEMA.MONTHLY_REVENUE  YEAR, MONTH, REVENUE  {partitionsTotal=1, partitionsAssigned=1, bytesAssigned=2560}\n",
      "\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "snow_df_spend_and_revenue_per_month.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Transformed Data into Snowflake Table\n",
    "\n",
    "Let's save the transformed data into a Snowflake table SPEND_AND_REVENUE_PER_MONTH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('SPEND_AND_REVENUE_PER_MONTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation: Run Monthly Revenue Data Transformations As a Snowflake Task (DAG)\n",
    "\n",
    "Note: Optionally you can run all these transformations as an automated task by deploying the code to Snowflake as a Snowpark Stored Procedure and executing it as a Snowflake Task. By using a DAG we can run it AFTER campaign_spend_data_pipeline_task.\n",
    "\n",
    "TIP: Learn more about Stored Procedures and Snowflake Tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_revenue_data_pipeline(session: Session) -> str:\n",
    "  # Load revenue table and transform the data into revenue per year/month using group_by and agg() functions\n",
    "  snow_df_spend_per_month_t = session.table('spend_per_month')\n",
    "  snow_df_revenue_t = session.table('monthly_revenue')\n",
    "  snow_df_revenue_per_month_t = snow_df_revenue_t.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n",
    "\n",
    "  # Join revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training\n",
    "  snow_df_spend_and_revenue_per_month_t = snow_df_spend_per_month_t.join(snow_df_revenue_per_month_t, [\"YEAR\",\"MONTH\"])\n",
    "\n",
    "  # SAVE in a new table for the next task\n",
    "  snow_df_spend_and_revenue_per_month_t.write.mode('overwrite').save_as_table('SPEND_AND_REVENUE_PER_MONTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since monthly_revenue_data_pipeline is depended on that campaign_spend_data_pipeline is executed first we want to create a DAG to make sure they run in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the previous task\n",
    "task_res.delete()\n",
    "\n",
    "with DAG(\"de_pipeline_dag\", schedule=timedelta(minutes=3)) as dag:\n",
    "    # Create a task that runs our first pipeline\n",
    "    dag_spend_task = DAGTask(name='campaign_spend_data_pipeline_task'\n",
    "                        , definition=StoredProcedureCall(\n",
    "                                    campaign_spend_data_pipeline, stage_location='@dash_sprocs'\n",
    "                                )\n",
    "                        ,warehouse='DASH_L1'\n",
    "                        )\n",
    "    # Create a task that runs our second pipleine\n",
    "    dag_revenue_task = DAGTask(name='monthly_revenue_data_pipeline'\n",
    "                          , definition=StoredProcedureCall(\n",
    "                                monthly_revenue_data_pipeline, stage_location='@dash_sprocs'\n",
    "                            )\n",
    "                        ,warehouse='DASH_L1'\n",
    "                        )\n",
    "# Shift right and left operators can specify task relationships.\n",
    "dag_spend_task >> dag_revenue_task  # dag_spend_task is a predecessor of dag_revenue_task\n",
    "\n",
    "schema = root.databases[session.get_current_database()].schemas[session.get_current_schema()]\n",
    "dag_op = DAGOperation(schema)\n",
    "\n",
    "dag_op.deploy(dag,mode=CreateMode.or_replace)\n",
    "\n",
    "# A DAG is not suspened by default so we will supend the root task that will suspend the full DAG\n",
    "root_task = tasks[\"DE_PIPELINE_DAG\"]\n",
    "root_task.suspend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run DAG\n",
    "\n",
    "Note that we can manually run DAGs even if they are suspended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dag_op.run(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suspend Tasks\n",
    "\n",
    "Note: For the sake of this lab, if you resume the above tasks, suspend them to avoid unecessary resource utilization by executing the following commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_task = tasks[\"DE_PIPELINE_DAG\"]\n",
    "# root_task.suspend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
