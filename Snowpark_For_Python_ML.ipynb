{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "Perform data analysis and data preparation tasks to train a Linear Regression model to predict future ROI (Return On Investment) of variable ad spend budgets across multiple channels including search, video, social media, and email using Snowpark for Python, Snowpark ML and Streamlit. By the end of the session, you will have an interactive web application deployed visualizing the ROI of different allocated advertising spend budgets.\n",
    "\n",
    "***Prerequisite**: Before proceeding with this Notebook, you must first successfully run through Snowpark_For_Python_DE.ipynb.*\n",
    "## Machine Learning\n",
    "\n",
    "In this Notebook, we will focus on Machine Learning in Snowflake using Snowpark for Python.\n",
    "\n",
    "- Establish secure connection to Snowflake\n",
    "- Load features and target from Snowflake table into Snowpark DataFrame\n",
    "- Prepare features for model training\n",
    "- Train ML model using Snowpark ML in Snowflake and upload the model to Snowflake stage\n",
    "- Register ML model and use it for inference from Snowpark ML Model Registry (currently in public preview)\n",
    "    - Note: The ML Model is called from the Streamlit Apps. See [Snowpark_Streamlit_Revenue_Prediction.py](https://github.com/Snowflake-Labs/sfguide-getting-started-dataengineering-ml-snowpark-python/blob/5dd84401337fab20fd8bf95eee3c70b0b2d11809//Snowpark_Streamlit_Revenue_Prediction.py) and [Snowpark_Streamlit_Revenue_Prediction_SiS.py](https://github.com/Snowflake-Labs/sfguide-getting-started-dataengineering-ml-snowpark-python/blob/5dd84401337fab20fd8bf95eee3c70b0b2d11809//Snowpark_Streamlit_Revenue_Prediction_SiS.py)\n",
    "  \n",
    "For environment setup including loading data into Snowflake tables, and step-by-step instructions, please refer to the [QuickStart Guide](https://quickstarts.snowflake.com/guide/getting_started_with_dataengineering_ml_using_snowpark_python/index.html#0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.modeling.compose import ColumnTransformer\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from snowflake.ml.modeling.linear_model import LinearRegression\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.version import VERSION as ml_version\n",
    "\n",
    "# Misc\n",
    "#import pandas as pd\n",
    "import json\n",
    "import logging \n",
    "logger = logging.getLogger(\"snowflake.snowpark.session\")\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish Secure Connection to Snowflake\n",
    "\n",
    "Using the Snowpark Python API, itâ€™s quick and easy to establish a secure connection between Snowflake and Notebook.\n",
    "\n",
    "Note: Other connection options include Username/Password, MFA, OAuth, Okta, SSO\n",
    "\n",
    "TIP: Learn more about [Session](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/session) object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                        : TRAININGSNOWFLAKE\n",
      "Role                        : \"ACCOUNTADMIN\"\n",
      "Database                    : \"DASH_DB\"\n",
      "Schema                      : \"DASH_SCHEMA\"\n",
      "Warehouse                   : \"DASH_L1\"\n",
      "Snowflake version           : 8.12.2\n",
      "Snowpark for Python version : 1.13.0\n",
      "Snowpark ML version         : 1.2.3\n"
     ]
    }
   ],
   "source": [
    "# Create Snowflake Session object\n",
    "connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('select current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n",
    "print('Snowpark ML version         : {}'.format(ml_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target\n",
    "\n",
    "At this point we are ready to perform the following actions to save features and target for model training.\n",
    "\n",
    "- Delete rows with missing values\n",
    "- Exclude columns we don't need for modeling\n",
    "- Save features into a Snowflake table called MARKETING_BUDGETS_FEATURES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"REVENUE\"   |\n",
      "---------------------------------------------------------------------\n",
      "|516431           |517618          |516729   |517208   |3264300.11  |\n",
      "|506497           |504679          |501098   |501947   |3208482.33  |\n",
      "|522780           |521395          |522762   |518405   |3311966.98  |\n",
      "|519959           |520537          |520685   |521584   |3311752.81  |\n",
      "|507211           |507404          |511364   |507363   |3208563.06  |\n",
      "|518942           |520863          |522768   |519950   |3334028.46  |\n",
      "|505715           |505221          |505292   |503748   |3185894.64  |\n",
      "|520148           |520711          |521427   |520724   |3334570.96  |\n",
      "|522151           |518635          |520583   |521167   |3316455.44  |\n",
      "|467736           |474679          |469856   |469784   |2995042.21  |\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "snow_df_spend_and_revenue_per_month = session.table('spend_and_revenue_per_month')\n",
    "\n",
    "# Delete rows with missing values\n",
    "snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.dropna()\n",
    "\n",
    "# Exclude columns we don't need for modeling\n",
    "snow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.drop(['YEAR','MONTH'])\n",
    "\n",
    "# Save features into a Snowflake table call MARKETING_BUDGETS_FEATURES\n",
    "snow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('MARKETING_BUDGETS_FEATURES')\n",
    "snow_df_spend_and_revenue_per_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training using Snowpark ML in Snowflake\n",
    "\n",
    "Learn more about Snowpark ML.\n",
    "\n",
    "NOTE: For workloads that require a large amount of memory and compute resources, consider using [Snowpark-Optimized Warehouses](https://docs.snowflake.com/en/developer-guide/snowpark/python/python-snowpark-training-ml#snowpark-optimized-warehouses).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on Train : 0.995107670224394\n",
      "R2 score on Test  : 0.9069317995589423\n"
     ]
    }
   ],
   "source": [
    "CROSS_VALIDATION_FOLDS = 10\n",
    "POLYNOMIAL_FEATURES_DEGREE = 2\n",
    "\n",
    "# Create train and test Snowpark DataDrames\n",
    "train_df, test_df = session.table(\"MARKETING_BUDGETS_FEATURES\").random_split(weights=[0.8, 0.2], seed=0)\n",
    "\n",
    "# Preprocess the Numeric columns\n",
    "# We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns\n",
    "# NOTE: High degrees can cause overfitting.\n",
    "numeric_features = ['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\n",
    "numeric_transformer = Pipeline(steps=[('poly',PolynomialFeatures(degree = POLYNOMIAL_FEATURES_DEGREE)),('scaler', StandardScaler())])\n",
    "\n",
    "# Combine the preprocessed step together using the Column Transformer module\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)])\n",
    "\n",
    "# The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', LinearRegression())])\n",
    "parameteres = {}\n",
    "\n",
    "# Use GridSearch to find the best fitting model based on number_of_folds folds\n",
    "model = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=parameteres,\n",
    "    cv=CROSS_VALIDATION_FOLDS,\n",
    "    label_cols=[\"REVENUE\"],\n",
    "    output_cols=[\"PREDICTED_REVENUE\"],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit and Score\n",
    "model.fit(train_df)\n",
    "train_r2_score = model.score(train_df)\n",
    "test_r2_score = model.score(test_df)\n",
    "\n",
    "# R2 score on train and test datasets\n",
    "print(f\"R2 score on Train : {train_r2_score}\")\n",
    "print(f\"R2 score on Test  : {test_r2_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Trained Model to Snowflake Model Registry\n",
    "\n",
    "The Model Registry allows to store models as objects in a schema in Snowflake. Note that by default the database and schema of the session is used.\n",
    "\n",
    "Learn more about [Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = Registry(session)\n",
    "MODEL_NAME = \"PREDICT_ROI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you try to log the model with the same name, you may get \"ValueError: (0000) Model PREDICT_ROI version v1 already existed.\" error. \n",
    "# If that's the case, uncomment and run this cell.\n",
    "\n",
    "# registry.delete_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = registry.log_model(model,\n",
    "                        model_name=MODEL_NAME,\n",
    "                        version_name=\"v1\",\n",
    "                        metrics={\"R2_train\": train_r2_score, \"R2_test\":test_r2_score},\n",
    "                        comment='Model pipeline to predict revenue',\n",
    "                        options={\"embed_local_ml_library\": True}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Logged Model in Snowflake Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>owner</th>\n",
       "      <th>default_version_name</th>\n",
       "      <th>versions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-02 09:13:03.543000-07:00</td>\n",
       "      <td>PREDICT_ROI</td>\n",
       "      <td>DASH_DB</td>\n",
       "      <td>DASH_SCHEMA</td>\n",
       "      <td>None</td>\n",
       "      <td>ACCOUNTADMIN</td>\n",
       "      <td>V1</td>\n",
       "      <td>[\"V1\"]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on         name database_name  schema_name  \\\n",
       "0 2024-04-02 09:13:03.543000-07:00  PREDICT_ROI       DASH_DB  DASH_SCHEMA   \n",
       "\n",
       "  comment         owner default_version_name versions  \n",
       "0    None  ACCOUNTADMIN                   V1   [\"V1\"]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.show_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Once the model is logged we can use it for inference on new data.\n",
    "\n",
    "First we will create a Snowpark DataFrame with some sample data and then call the logged model to get new predictions. Note: we will handle negative values in our Streamlit application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "|\"SEARCH_ENGINE\"  |\"SOCIAL_MEDIA\"  |\"VIDEO\"  |\"EMAIL\"  |\"PREDICTED_REVENUE\"  |\n",
      "------------------------------------------------------------------------------\n",
      "|8500             |9500            |2000     |500      |-107982.7042556526   |\n",
      "|250000           |250000          |200000   |450000   |-12735711.132665249  |\n",
      "|500000           |500000          |500000   |500000   |3180176.6792253554   |\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = session.create_dataframe([[250000,250000,200000,450000],[500000,500000,500000,500000],[8500,9500,2000,500]], \n",
    "                                    schema=['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL'])\n",
    "mv.run(test_df, function_name='predict').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tasks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m root_task \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDE_PIPELINE_DAG\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m root_task\u001b[38;5;241m.\u001b[39msuspend()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tasks' is not defined"
     ]
    }
   ],
   "source": [
    "root_task = tasks[\"DE_PIPELINE_DAG\"]\n",
    "root_task.suspend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
